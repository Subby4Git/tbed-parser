%
% File naaclhlt2009.tex
%
% Contact: nasmith@cs.cmu.edu

\documentclass[11pt]{article}
\usepackage{naaclhlt2009}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\citet}[1]{\cite{#1}}
\newcommand{\citep}[1]{\cite{#1}}
\newcommand{\ut}{\textit}
\newcommand{\lit}[1]{\footnote{Lit. translation: #1}}

\newcommand{\efgr}[2]{
  \begin{figure}[htbp]
    \makebox[8.5cm]{\framebox[5cm]{\rule{0pt}{5cm}}}
    \caption{#2}
    \label{#1}
  \end{figure}
}

\newcommand{\fgrparam}[4]{
  \begin{figure}[htbp]
    \begin{center}
      \leavevmode
      \includegraphics[#1]{#2}
    \end{center}
    \caption{\textit{#4}}
    \label{#3}
  \end{figure}
}

\title{Semantic Transformation-based Error-driven Parser}

\author{Filip Jurcicek, XXX \\
  Department of Engineering \\
  University of Cambridge \\
  {\tt fj228@cam.ac.uk, XXX}}

\date{}

\begin{document}
\maketitle
\begin{abstract}a
  In this paper, we present a semantic parser which transforms initial naive semantic hypothesis into correct semantics by using a ordered set of rules. These rules are learnt automatically from the training corpus with no linguistic knowledge.
\end{abstract}

\section{Introduction}


\section{Related work}

Inductive logic programming - Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing - Tang and Mooney (2001)

Transformation-based Error-driven Learning - Some Advances in Transformation-Based Part of Speech Tagging - Brill (1994)
Learning to Transform Natural to Formal Languages - Kate, Wong and Mooney (2005)

Subsection~\ref{ssec:first}).

\section{Transformation-based parser}
This section describes the transformation-based parser. First of all, we desribe rule templates used to generate rules for the rule inference process. Secondly, we describe the learning process. Finally, we describe parsing algorithm.

\subsection{Rule templates}
The learning argorithm uses rule templates to instantiate rules which are subsequently tested by the learning algrithm. Each rule tamplate is composed of a trigger and transformation. \textbf{A trigger controls whether a transformation of hypothesis can be perfomed}. Each trigger question either input sentence or output semantics. As a results each trigger contains one or several condition:
\begin{itemize}
  \item The sentence contains n-gram N?
  \item The sentence contains skiping\footnote{A skiping bigram is bigram which skips one or more words between words in the bigram} bigram B?
  \item The semantics dialogue act equals to D?
  \item The semantics containss slot S?
\end{itemize}
If a trigger contains more than one condition, then all conditions must be satisfied. \textbf{Get rid of the questions.}

A transformation performs one of these operation:
\begin{itemize}
  \item substitute a dialogue act type
  \item add a slot
  \item delete a slot
  \item substitute a slot
\end{itemize}
As substitution can either substitute a whole slot, an equal sign in the slot, or a slot name.

\subsection{Learning}

Rules are learned sequentially:
\begin{enumerate}
  \item initial semantics is assigned as hypothesis to each input sentence
  \item repeat as long as the number of errors\footnote{Number of errors include number of dialogue act substitutions, number of slot insertions, number of slot deletions, number of slot substitutions.} on training set decreases
  \begin{enumerate}
    \item generate all possible rules which correct at least one error in the training set
    \item measure number of corrected errors for each rule
    \item select the best rule
    \item apply the selected rule
  \end{enumerate}
  \time prune rules
\end{enumerate}

The make the parser more robust, we increase robustnes of the parser by the following steps. 

First, the number of plausible slot values for each slot is usually very high. As a result, we replace all lexical realizations from the database, available to \textbf{a dialogue manager}, by its slot name in the input sentence. For example, in sentence ``find all the flights from cincinnati to the new york city'' the lexical realization are replaced as follows: ``please find all the flights from city-0 to the city-1''. Similary, we replace slot values in the semantics.

Secondly, to limit overfiting the training data, we prune the rules which are learnt at the end of the learning. We sequentily apply each rule on the development set. And we chose the number of rules for which the parser gets the highest score on the development data.



First of all, very naive rules are lear are for eample 
Classifier learns to correct its errors
STEC can delete an incorrect slot
STEC can substitute
A slot name of an incorrect slot
An equal sign of an incorrect slot

\textbf{To speed up training, we select not only one best rule but also rules which correct at minimum 80\% errors of the best rule. }

\subsection{Parsing}
The semantic parser transforms initial naive semantic hypothesis into correct semantics by using a ordered set of rules and the parsing is composed of \textbf{three} steps: 
\begin{enumerate}
  \item initial semantics is assigned as hypothesis
  \item sequentialy apply all rules\footnote{Input sentence is not modified by rules. As a result, words from the sentence can be trigger sevaral different transformations.}
  \item output hypothesis semantics
\end{enumerate}






Although we use the 

% \begin{table}
% \begin{center}
% \begin{tabular}{|l|rl|}
% \hline \bf Type of Text & \bf Font Size & \bf Style \\ \hline
% paper title & 15 pt & bold \\
% \hline
% \end{tabular}
% \end{center}
% \caption{\label{font-table} Font guide. }
% \end{table}

\section{Evaluation}

In this section, we evaluate our parser on two distinct compora, and compare our results with the state-of-the-art techniques and handcrafted rule-based parser. 

\subsection{Datasets}

Our first dataset consists of tourist information dialogues in a fictitious
town (TownInfo). The dialogues were collected through user
trials in which users searched for information about a specific venue
by interacting with a dialogue system in a noisy background. These
dialogues were previously used for training dialogue management
strategies \cite{williams07,thomson08}. The semantic representation of the user utterance consists of a root dialogue act type and a set of slots which are either
unbound or associated with a child value. For example, ``What is
the address of Char Sue'' is represented as request(address='Char Sue'), and ``I would like a Chinese restaurant?'' as inform(food='Chinese',type='restaurant'). The TownInfo training, development, and test sets respectively contain
8396, 986 and 1023 transcribed utterances.  The data includes the transcription of the top hypothesis of the ATK speech recogniser,
which allows us to evaluate the robustness of our models to recognition
errors (word error rate = 34.4\%). 

In order to compare our results with previous work \cite{he06,zettlemoyer07},
we apply our method to the Air Travel Information System dataset
(ATIS) \cite{atis94}. This dataset consists of user requests for flight information, for example ``Find flight from San Diego to Phoenix on Monday is rerepresented as flight(from.city="San Diego",to.city="Phoenix",departure.day="Monday"). We use 5012 utterances for training, and the DEC94 dataset as develoment data. As in previous work, we test our method on the 448 utterances of the NOV93 dataset, and the evaluation criteria is the F-measure of the number of reference slot/value pairs that appear in the output semantic (e.g., from.city = New York). He \& Young detail the test data extraction process in \cite{he05}.

For both corpora are available databases with lexical entries for slot values e.g. city names, airport names, etc. 

\subsection{Results}

We also compare our models with the handcrafted Phoenix grammar \cite{ward91} used in the trials \cite{williams07,thomson08}. The Phoenix parser implements a partial matching algorithm that was designed for robust spoken language understanding.

\section{Discussion}

\efgr{fig:learning:curve}{The learning curve shows the relation between number of learnt rules and the F-measure for both TI and ATIS corpora.}

The number of learnt rules is very small. As is shown in the figure \ref{fig:learning:curve}, learning curves for both training data and development data are very steep. Although our current strategy for choosing the final number of rules for decoding is to keep only the rules for which we obtain highest F-measure on the development data, we could use much less rules without scarifying accuracy. For example, we accepted 0.1\% lower F-measure on the development data than we would need only YYY rules in comparison with XXX rules if select the number of rules based in the highest F-measure. In contrast, the initial lexicon the CCG parser \cite{zettlemoyer07} contains about 180 sometimes very complex entries for general English and yet additional lexical entries must be learnt.

Also, the number of rules per semantic concept (dialogue act or slot name) is very low. In TI data, we have XXX different dialogue acts and XXX slot and the average number of rules per semantic concept is XXX. In case of ATIS data, we have XXX dialogue acts and XXX slots and the average number of rules per semantic concept is XXX.

Lexical realizations of a slot can overlap with lexical realization of neighbouring slots. It is shows to be important paternt, for example in the trigram (city-0,and,city-1) is very comon for sentce including ''between city-0 and city-1``. The lexical realizations city-0, city-1 respectively would be classified as from.city, and city-1 just becasue we know the  

\section*{Acknowledgments}

Do not number the acknowledgment section.


\begin{thebibliography}{}

\bibitem[\protect\citename{Williams and Young}2007]{williams07}
J. Williams and S. Young, 
\newblock 2007.
\newblock {\em Partially observable markov decision
processes for spoken dialog systems}.
\newblock Computer Speech and Language, vol. 21, no. 2, pp. 231-422.

\bibitem[\protect\citename{Thomson et al}2008]{thomson08}
B. Thomson, M. Gasic, S. Keizer, F. Mairesse, J. Schatzmann,
K. Yu, and S. Young.
\newblock 2008.
\newblock {\em User study of the Bayesian update of dialogue state approach to dialogue management}.
\newblock Proceedings of Interspeech.

\bibitem[\protect\citename{Ward}1991]{ward91}
W. H. Ward.
\newblock 1991.
\newblock {\em The Phoenix system: Understanding spontaneous
speech}.
\newblock Proceedings of ICASSP.

\bibitem[\protect\citename{He and Young}2005]{he05}
Yulan He and Steve Young.
\newblock 2005.
\newblock {\em Semantic processing using the
hidden vector state model}.
\newblock Computer Speech \& Language,vol. 19, no. 1, pp. 85-106.

\bibitem[\protect\citename{Zettlemoyer and Collins}2007]{zettlemoyer07}
Luke S. Zettlemoyer and Michael Collins.
\newblock 2005.
\newblock {\em Online learning of relaxed CCG grammars for parsing to logical form}.
\newblock Proceedings of EMNLP-CoNLL.

\bibitem[\protect\citename{Dahl et al}1994]{atis94}
D. A. Dahl, M. Bates, M. Brown,W. Fisher, K. Hunicke-Smith,
D. Pallett, C. Pao, A. Rudnicky, and E. Shriberg.
\newblock 1994.
\newblock {\em Expanding the scope of the ATIS task: The ATIS-3 corpus}.
\newblock Proceedings of the ARPA HLT Workshop.

\bibitem[\protect\citename{He and Young}2006]{he06}
Yulan He and Steve Young.
\newblock 2006.
\newblock {\em Spoken language understanding using the hidden vector state model}.
\newblock Computer Speech \& Language, vol. 19, no. 1, pp. 85-106.

\end{thebibliography}

\end{document}
