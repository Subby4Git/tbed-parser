%
% File naaclhlt2009.tex
%
% Contact: nasmith@cs.cmu.edu

\documentclass[11pt]{article}
\usepackage{naaclhlt2009}
\usepackage{times}
\usepackage{latexsym}
\usepackage{epsfig}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\citet}[1]{\cite{#1}}
\newcommand{\citep}[1]{\cite{#1}}
\newcommand{\ut}{\textit}
\newcommand{\lit}[1]{\footnote{Lit. translation: #1}}

\newcommand{\efgr}[2]{
  \begin{figure}[htbp]
    \makebox[8.5cm]{\framebox[5cm]{\rule{0pt}{5cm}}}
    \caption{#2}
    \label{#1}
  \end{figure}
}

\newcommand{\fgrparam}[4]{
  \begin{figure}[htbp]
    \begin{center}
      \leavevmode
      \includegraphics[#1]{#2}
    \end{center}
    \caption{#4}
    \label{#3}
  \end{figure}
}

\title{Error Corrective Learning for Semantic parsing}

\author{F. Jur\v{c}\'{i}\v{c}ek, F. Mairesse, M. Ga\v{s}i\'{c}, S. Keizer, B. Thomson, K. Yu, and S. Young \\
\\
Engineering Department \\
Cambridge University \\
Trumpington Street, Cambridge, CB2 1PZ, UK \\
\\
{\tt \{fj228, farm2, mg436, sk561, brmt2, ky219, sjy\}@eng.cam.ac.uk}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}a
  In this paper, we present a semantic parser which transforms initial naive semantic hypothesis into correct semantics by using an ordered set of rules. These rules are learned automatically from the training corpus with no linguistic knowledge.
\end{abstract}

\section{Introduction}

\textbf{Check how you call STEP/ECL parser!}

Semantic parsing is important part of a spoken dialogue system \cite{williams07,thomson08}. The goal of a semantic parsing is to construct formal meaning representation (semantics) which is directly executable by a dialogue manager. Such semantics is usually defined by a grammar, e.g. LR grammar for GeoQuery domain \cite{wong06} \textbf{better paper???}, which is designed by a domain expert and easy to interpret by a dialogue manager or question answering system.

Semantic parsing can be understood as machine translation from a natural language to a formal language. First, we do not not have formal grammar for natural language is ungrammatical, include hesitations, and very often only fragments of complete sentences, e.g. ``Boston to Miami tomorrow''. 

In our approach, we adapt transformation-based
learning (TBL) \cite{brill95} to the problem
of semantic parsing \textbf{slot classification, attribute/value pair extraction}. TBL attempts to find an
ordered list of transformation rules to improve a baseline annotation.

The rules decompose the search space into
a set of consecutive words (windows) within which
alignment links are added, to or deleted from, the
initial alignment. 

TBL is an appropriate choice for this problem for
the following reasons:
1. It can be optimized directly with respect to an
evaluation metric.
2. It learns rules that improve the initial prediction
iteratively, so that it is capable of correcting
previous errors in subsequent iterations.
3. It provides a readable description (or classification)
of errors made by the initial system,
thereby enabling alignment refinements.

The rest of the paper is organized as follows: In
the next section we describe previous work on semantic parsing. Section \ref{sec:tbl:example} presents am example of TBL based semantic parsing. Section \ref{sec:tbl:learning} describes the learning process. Section
\ref{sec:evaluation} compares ECL to the previously developed semantic parser on ATIS \cite{atis94} and TownInfo \cite{williams07,thomson08} tasks.

We show that ECL is competitive to the state-of-the-art semantic parser on the ATIS task without using any handcrafted linguistic knowledge.

\textbf{Define slot, slot name and slot value. Give an example of such slot} 

\section{Related work}

In Section \ref{sec:evaluation}, we will compare performance of our method the four existing systems that were evaluated on the same dataset we consider. 
Support vector machines \cite{mairesse09} have been used to build a semantic trees by recursive calling classifiers to estimate probabilities of production rules using a linear kernel and word based featueres. 

Markov logic networks \cite{meza08a,meza08b} have been used to extract-slot vales by applying ideas of a Markov network to first-order logic. In this approach, weights are attached to first-order clauses which represent relitonhip between slot names and its values. Such weighted clauses are used as templates for features of Markov networks. 
Automatic induction of combinatory categorical grammar \cite{zettlemoyer07} have been used to map sentences to lambda-calculus. The combinatry categorical grammar is generalized into probabilistic model by learning log-linear model. An online learning algorithm update weights of features representing a parse tree of an input sentence. They show that their technique produces the state-of-the-art performance on the Air Travel Information (ATIS) dataset \cite{atis94}.
\textbf{However, }apart from using the teh lexcical categories (city names, airport names, etc) readily available from the ATIS corpus, they also need considerable number of handcrafted entries in their initial lexicon. 

\cite{he06} developed a parser for the ATIS domain also takes utterances paired with semantics as input. Their parser approximate a pushdown automaton with semantic concepts as non-terminal symbols.


There has been done a large amount of reasearch that is not directly comparable to our because they require different levels of supervision. Wong \cite{wong06} used machine translation techniques with a syntax-based translation model based on the synchronus context-free grammars. 
Inductive logic programing \cite{tang01} have been used to incrementaly develop a theory including a set of predicates. In each iteration, the predicates were generalized from predicates in the theory and predicates automaticaly constructed from examples. 
Transformation techniques \cite{kate05} have been used to sequentially rewrite an utterance into semantics. Our approach differ in the way how the semantics is constructed. Instead of rewriting an utterance, we transform initial naive semantic hypothesis. As a result, we can use in input words several times to trigger transformations of the output. This extends our ability to handle non-compositionality phenomena in spoken language.
Kate \cite{kate08} used suport vector machines and tree kernels to integrate knowledge contained in the dependeny tries to capture long-range long-range relationship between words. 


\section{Transformation-based parser}
This section describes the transformation-based parser. First of all, we describe rule templates used to generate rules for the rule inference process. Secondly, we describe the learning process. Finally, we describe parsing algorithm.

\textbf{TBL chooses the transformation that reduces the errors the most.}

\textbf{
Existence of initial annotator.
Transformations are applied in sequence. 
Results of previous transformations are visible to following transformations.
Existence of current label.
It can handle dynamic problems well.
}

\textbf{Most of the effective methods can be roughly divided into rule-based and probabilistic algorithms. In general, the rule-based methods have the advantage of capturing the necessary information in a small and concise set of rules. For example, in part-of-speech tagging rule-based and probabilistic methods achieve comparable accuracies, but rule-based methods capture the knowledge in a hundreds or so simple rules, while the probabilistic methods have very high-dimensional parameter space (millions of parameters)}

\subsection{Rule templates}
The learning algorithm uses rule templates to instantiate rules which are subsequently tested by the learning algorithm. Each rule template is composed of a trigger and transformation. \textbf{A trigger controls whether a transformation of hypothesis can be performed}. Each trigger question either input utterance or output semantics. As a results each trigger contains one or several condition:
\begin{itemize}
  \item The utterance contains n-gram N?
  \item The utterance contains skipping\footnote{A skipping bigram is bigram which skips one or more words between words in the bigram} bigram B?
  \item The semantics dialogue act equals to D?
  \item The semantics contains slot S?
\end{itemize}
If a trigger contains more than one condition, then all conditions must be satisfied. \textbf{Get rid of the questions.}

A transformation performs one of these operation:
\begin{itemize}
  \item substitute a dialogue act type
  \item add a slot
  \item delete a slot
  \item substitute a slot
\end{itemize}
As substitution can either substitute a whole slot, an equal sign in the slot, or a slot name.

\subsection{Example of Parsing} \label{sec:tbl:example}
The semantic parser transforms initial naive semantic hypothesis into correct semantics by applying rules from an ordered set of rules. 

The parsing consists of \textbf{three} steps: 
\begin{enumerate}
  \item initial semantics is assigned as hypothesis
  \item sequentially apply all rules\footnote{Input utterance is not modified by rules. As a result, words from the utterance can be trigger several different transformations.}
  \item output hypothesis semantics
\end{enumerate}

We demonstrate the parsing on an example. Think of the utterance: \textit{``find all the flights between toronto and san diego that arrive on saturday''} 

First, the goal ``flight'' is used as the initial goal because it is the most common goal in the ATIS dataset and no slots are added in the semantics.

\vspace{.25cm}
\begin{tabular}{lll}
  GOAL & = & flight
\end{tabular} 
\vspace{.25cm}

Secondly, the rules whose triggers match the sentence and the semantic hypothesis are sequentially applied.

\vspace{.25cm}
\begin{tabular}{ll}
  trigger & transformation \\
  \hline 
  ``between               & add slot \\
    toronto and''         &``from.city=Toronto'' \\
  ``and san diego''       & add slot \\
                          & ``to.city=San Diego'' \\
  ``saturday''            & add slot \\
                          & ``departure.day=Saturday'' \\
\end{tabular} 
\vspace{.25cm}

The trigger \textbf{``and Sand Diego'' is example of non-compositionality}, in which the words in an utterance do not have a one-to-one correspondence with the slots in the semantics. The word ``and'' is used two times and in the second time it indicate that the city ``San Diego'' is slot value of the slot ``to.city''. After application of the transformations, we obtain the following semantic hypothesis. 

\vspace{.25cm}
\begin{tabular}{lll}
  GOAL          & = & flight \\
  from.city     & = & Toronto \\
  to.city       & = & San Diego \\
  departure.day & = & Saturday \\
\end{tabular} 
\vspace{.25cm}

The word ``saturday'' is incorrectly classified because the date and time values are associated with slot ``departure*'' most of the time.  As TBL \textbf{classifier} learns to correct its errors, the parser also applies the error correcting rules. In our case it is:

\vspace{.25cm}
\begin{tabular}{ll}
  trigger & transformation \\
  \hline 
  ``arrive''            & substitute slot from\\
                        & ``departure.day=*'' to \\
                        & ``arrive.day=*'' \\
\end{tabular} 
\vspace{.25cm}

The final semantic hypothesis is the following.

\vspace{.25cm}
\begin{tabular}{lll}
  GOAL          & = & flight \\
  from.city     & = & Toronto \\
  to.city       & = & San Diego \\
  saturday.day & = & Saturday \\
\end{tabular} 
\vspace{.25cm}

Up to now, we considered the utterance as a bag of words and no notion of locality was considered. To implement this, we constrain triggers to be activated only if they are in vicinity of the slot lexical ralization. Only the rule substituting slot is constrained. For example, before we perform the substitution of the slot ``departure.day'' to ``arrival.day'', we test whether word ``arrive'' is in the vicinity of the slot. The reason is that we do not want to trigger the substitution of the slot ``from.city=Toronto'' to ``to.city=Toronto'' because the parser also learns rule.

\vspace{.25cm}
\begin{tabular}{ll}
  trigger & transformation \\
  \hline 
  ``arrive''            & substitute slot from\\
                        & ``from.city=*'' to \\
                        & ``arrive.city=*'' \\
\end{tabular} 
\vspace{.25cm}

\fgrparam{width=8cm}{./fig/words-slots-alignment.pdf}{fig:alignment}{Alignment between words and the slots in the utterance.}

To implemet such constraints, we track the words from the utterance, which were used in triggers. Anytime we apply add or substitute transformation of a slot, we keep links to the words which triggered the transformation. Because a slot is removed when a delete transformation is applied, no tracking information is kept in this case. 

In Figure \ref{fig:alignment} (a), we see the alignment between the words and the slots in the utterance after applying the first set of rules. The full lines denote direct alignment created by transformations. The dasehed lines denote derived alignment - alignment derived from the direct alignment. Because no rules were trigered by words ``find all the flights'' and ``that arrive on'' those words could not be alligned directly to any of the slot, we have to derive such alignment. To compute derived alignment, first we order the slots so that the slot aligned with the left-most words is the first and the ordering results in minimum instances of crossing of direct alignment. Than every unaligned word is aligned with the nearest left and the right slot. In Figure\ref{fig:alignment} (a), the phrase ``find all the flights'' can be aligned to the slot ``from.city=Toronto'' only. The phrase ``that arrive on'' can be aligned to two slots ``to.city=San Dieago'' and ``departure.day=Saturday''.

In Figure \ref{fig:alignment} (b), we see the alignment after applying the substitution. We can see change in the alignment of the ``that arrive on''. First, the word ``arrive'' is alligned to the slot ``arrive.day=Saturday''. Second, the word ``on'' is aligned to the same slot ast word ``arrive'' because  otherwise the derived alignmet would cross the direct alignment. Finally, there is no change in the alignment of the word ``that''.


\subsection{Learning} \label{sec:tbl:learning}
\textbf{The central idea of transformation-based learning is to learn and ordered list of rules which progressively improve upon the current state of the training set.} 

The in initial assignment is made based on simple statistics, and then the rules are greedily learned to correct the mistakes, until no improvement can be made.

Using TBL approach to solve a classification problem assumes the existence of:


\begin{itemize}
  \item An initial call assignment. This can be done by assigning the most common simple semantics, e.g. inform().
  \item A set of templates for rules. These templates determine the predicates the rules will test, and they have the largest impact on the performance of the classifier.
  \item An objective function for learning. The typical objective function is the difference in performance resulting from applying the rule.
\end{itemize}

At the beginning of the learning phase, the training set is first given an initial class assignment. The system then iteratively executes the following steps:

\begin{enumerate}
  \item Generate all productive rules.
  \item For each rule:
  \begin{enumerate}
    \item Apply to a copy of the most recent state of the training set.
    \item Score the result using the objective function.
  \end{enumerate}
  \item Select the rule with the best score.
  \item Apply the rule  to the current state of the training set. updating it to reflect the change.
  \item Stop if the score is smaller than some pre-set threshold T
  \item Repeat from the step 1.
\end{enumerate}

\textbf{Th system learns a list of rules in a greedy fashion, according the objective function. When no rule that improves the current state of the training set beyond the pre-set can be found, the training phase ends.}

During the decoding phase, the test set is initialized with the same initial class's assignment. Each rule is than applied, in the order it was learned, to the test set. The final classification is the one attained when all rules have been applied.


Rules are learned sequentially:
\begin{enumerate}
  \item initial semantics is assigned as hypothesis to each utterance
  \item repeat as long as the number of errors\footnote{Number of errors include number of dialogue act substitutions, number of slot insertions, number of slot deletions, number of slot substitutions.} on training set decreases
  \begin{enumerate}
    \item generate all possible rules which correct at least one error in the training set
    \item measure number of corrected errors for each rule
    \item select the best rule
    \item apply the selected rule
  \end{enumerate}
  \item prune rules
\end{enumerate}

The make the parser more robust, we increase robustness of the parser by the following steps. 

First, the number of plausible slot values for each slot is usually very high. As a result, we replace all lexical realizations from the database, available to \textbf{a dialogue manager}, by its slot name in the utterance. For example, in utterance ``find all the flights from cincinnati to the new york city'' the lexical realization are replaced as follows: ``please find all the flights from city-0 to the city-1''. Similarly, we replace slot values in the semantics.

Secondly, to limit overfitting the training data, we prune the rules which are learned at the end of the learning. We sequentially apply each rule on the development set. And we chose the number of rules for which the parser gets the highest score on the development data.


First of all, very naive rules are learn are for example 
Classifier learns to correct its errors
STEC can delete an incorrect slot
STEC can substitute
A slot name of an incorrect slot
An equal sign of an incorrect slot

\textbf{To speed up the training process, we select multiple best performing rules and the performance of worst selected rule has to be at least at least 80\% of the best rule.}



\section{Evaluation} \label{sec:evaluation}

In this section, we evaluate our parser on two distinct corpora, and compare our results with the state-of-the-art techniques and handcrafted rule-based parser. 

\subsection{Datasets}

Our first dataset consists of tourist information dialogues in a fictitious
town (TownInfo). The dialogues were collected through user
trials in which users searched for information about a specific venue
by interacting with a dialogue system in a noisy background. These
dialogues were previously used for training dialogue management
strategies \cite{williams07,thomson08}. The semantic representation of the user utterance consists of a root dialogue act type and a set of slots which are either
unbound or associated with a child value. For example, ``What is
the address of Char Sue'' is represented as request(address='Char Sue'), and ``I would like a Chinese restaurant?'' as inform(food='Chinese',type='restaurant'). The TownInfo training, development, and test sets respectively contain
8396, 986 and 1023 transcribed utterances.  The data includes the transcription of the top hypothesis of the ATK speech recogniser,
which allows us to evaluate the robustness of our models to recognition
errors (word error rate = 34.4\%). 

In order to compare our results with previous work \cite{he06,zettlemoyer07},
we apply our method to the Air Travel Information System dataset
(ATIS) \cite{atis94}. This dataset consists of user requests for flight information, for example ``Find flight from San Diego to Phoenix on Monday is rerepresented as flight(from.city="San Diego",to.city="Phoenix",departure.day="Monday"). We use 5012 utterances for training, and the DEC94 dataset as development data. As in previous work, we test our method on the 448 utterances of the NOV93 dataset, and the evaluation criteria is the F-measure of the number of reference slot/value pairs that appear in the output semantic (e.g., from.city = New York). He \& Young detail the test data extraction process in \cite{he05}.

For both corpora are available databases with lexical entries for slot values e.g. city names, airport names, etc. 

\subsection{Improving disambiguation of long-range dependencies}

\fgrparam{width=8cm}{./fig/dep-tree.pdf}{fig:dep:tree}{Dependency tree of the sentence ''Show the cheapest flights from New York to San Jose arriving before 7pm on Monday`` generated by the RASP parser \cite{rasp06}.}

Besides simple n-grams and skipping bigrams more complex lexical features can be used. \cite{kate08} used gold standard word dependencies to capture long-range relationship between words. At its simplest, dependencies tree is one of the most concise ways to describe language syntax. Essentially, each word is viewed as the dependent of one other word, with the exception of a single word which that is the root of the sentence. Kate showed that word dependencies significantly improve semantic parsing because long-range dependencies from an utterance tend to be local in a dependency tree. For example, the words ''arriving`` and ''Monday`` are neighbors in the dependency tree but they are four words apart in the sentence (see Figure \ref{fig:dep:tree}).

Instead of using gold standard word dependencies, we used dependencies provided by RASP dependency parser \cite{rasp06}. First of all, we had to add capitalization and punctuation into the ATIS data to be able to use the RASP parser. The RASP parser without proper capitalization fails to tag ''new`` and ''york`` as NP and instead of this it tags ''new`` as ''JJ`` and 'york' as NP and the dependencies generated by the parser are unsatisfactory. Secondly, we generated new n-gram features from dependency trees. Even though the dependencies generated the RASP parser are no absolutely accurate, the new features increase performance in F-measure on ATIS data. 

Secondly, we generated long-range features by using POS tags\footnote{We used POS tags provided by the RASP parser; however, any POS tagger can be used instead.} 
Our motivation was work of \cite{meza08a,meza08b} who handcrafted features using words ''arrive``, ''arriving``, ''leave``, and ''leaving``. These handcrafted features disambiguate large number of semantic parsing errors in ATIS data because large portion or errors is caused by confusions between concepts ''arrival.time`` and ''departure.time``, ''arrival.day`` and ''departure.day``, etc. To generalize this approach, we want to automatically find features which could disambiguate words like ''Monday``, ''7pm``, and ''Boston``. As a result, we generate a new type of bigrams for a word and the nearest verb, preposition, etc. We use all parts-of-speech provided by RASP and the learning algorithm chose the most discriminative features. Among those learned are not only the words used by Meza-Rui but also words like ''stop``, ''reach``, ''buy`` and prepositions like ''at``, ''from``, ''to``, etc.

\textbf{Mention why I do not use tree similarity measure as Kate.}

\subsection{Results}

We also compare our models with the handcrafted Phoenix grammar \cite{ward91} used in the trials \cite{williams07,thomson08}. The Phoenix parser implements a partial matching algorithm that was designed for robust spoken language understanding.



\begin{table}
\begin{center}
\begin{tabular}{|l|ccc|}
\hline \makebox[2.99cm]{\bf Parser} & \makebox[1.1cm]{\bf Prec} & \makebox[1.1cm]{\bf Rec} & \bf F \\ \hline 
\multicolumn{4}{l}{\textbf{TownInfo dataset with transcribed utterances:}} \\
\hline
ECL      & 96.05 & 94.66 & 95.35 \\
STC      & 97.39 & 94.05 & \textbf{95.69} \\
Phoenix  & 96.33 & 94.22 & 95.26 \\
\hline
\multicolumn{4}{l}{\textbf{TownInfo dataset with ASR output:}} \\
\hline
ECL      & 92.72 & 83.42 & 87.82 \\
STC      & 94.03 & 83.73 & \textbf{88.58} \\
Phoenix & 90.28 & 79.49 & 84.54 \\
\hline
\multicolumn{4}{l}{\textbf{ATIS dataset with transcribed utterances:}} \\
\hline
ECL   & 96.37 & 95.12 & 95.74 \\
STC   & 96.73 & 92.37 & 94.50 \\
HVS   & - & - & 90.3  \\
MLN   & - & - & 92.99 \\
PCCG  & 95.11 & 96.71 & \textbf{95.9} \\
\hline
\end{tabular}
\end{center}
  \caption{\label{font-table} Slot/value precision (Prec), recall (Rec) and F-measure (F) for the ATIS and TownInfo datasets. ECL parser is compared with Phoenix parser and STC classifier \cite{mairesse09} on the TownInfo dataset and compared with HVS parser \cite{he06}, MLN parser \cite{meza08b}, STC classifier, and PCCG parser \cite{zettlemoyer07} on the ATIS dataset}
\end{table}

\section{Discussion}

\efgr{fig:learning:curve}{The learning curve shows the relation between number of learned rules and the F-measure for both TI and ATIS corpora.}

The number of learned rules is very small. As is shown in the figure \ref{fig:learning:curve}, learning curves for both training data and development data are very steep. Although our current strategy for choosing the final number of rules for decoding is to keep only the rules for which we obtain highest F-measure on the development data, we could use much less rules without scarifying accuracy. For example, we accepted 0.1\% lower F-measure on the development data than we would need only YYY rules in comparison with XXX rules if select the number of rules based in the highest F-measure. In contrast, the initial lexicon the CCG parser \cite{zettlemoyer07} contains about 180 complex entries for general English words or phrases and yet additional lexical entries must be learned. \textbf{explain better}

Also, the number of rules per semantic concept (dialogue act or slot name) is very low. In TI data, we have XXX different dialogue acts and XXX slot and the average number of rules per semantic concept is XXX. In case of ATIS data, we have XXX dialogue acts and XXX slots and the average number of rules per semantic concept is XXX.

Lexical realizations of a slot can overlap with lexical realization of neighbouring slots. It is shows to be important pattern, for example in the trigram (city-0,and,city-1) is very common for sentence including ''between city-0 and city-1``. The lexical realizations city-0, city-1 respectively would be classified as from.city, and city-1 just because we know the  


We found that the dialogue act type recognition accuracy of the STEP parser is lower than STC's; as a result, we tried to use SVM as STC does to classify dialogue act types. We believe that STC is better in dialogue act type recognition better because SVN classifier use all features at one time. STC makes decision in one step using all the features rather than making several decisions by several rules as STEP.

We hoped for an increase of F-measure as result of increased dialogue act type accuracy. However, we did not get any increase in F-measure.

\section{Conclusion}

Our approach adapts TBL to the problem of
word-level alignment by examining word features
as well as neighboring links.
 
\section*{Acknowledgments}

We would like to thank to Luke Zettlemoyer and I.V. Meza-Ruiz for their help with understanding their methods.



\begin{thebibliography}{}

\bibitem[\protect\citename{Brill}1995]{brill95}
E. Brill.
\newblock 1995.
\newblock {\em Transformation-based error-driven learning
and natural language processing: A case study in part-of-speech
tagging}.
\newblock Computational Linguistics, 21(4):543?565.

\bibitem[\protect\citename{Briscoe et al}2006]{rasp06}
E. Briscoe, J. Carroll and R. Watson.
\newblock 2006.
\newblock {\em The Second Release of the RASP System}.
\newblock Proceedings of COLING/ACL.

\bibitem[\protect\citename{Dahl et al}1994]{atis94}
D.A. Dahl, M. Bates, M. Brown, W. Fisher, K. Hunicke-Smith,
D. Pallett, C. Pao, A. Rudnicky, and E. Shriberg.
\newblock 1994.
\newblock {\em Expanding the scope of the ATIS task: The ATIS-3 corpus}.
\newblock Proceedings of the ARPA HLT Workshop.

\bibitem[\protect\citename{He and Young}2005]{he05}
Y. He and S. Young.
\newblock 2005.
\newblock {\em Semantic processing using the
hidden vector state model}.
\newblock Computer Speech \& Language,vol. 19, no. 1, pp. 85-106.

\bibitem[\protect\citename{He and Young}2006]{he06}
Y. He and S. Young.
\newblock 2006.
\newblock {\em Spoken language understanding using the hidden vector state model}.
\newblock Computer Speech \& Language, vol. 19, no. 1, pp. 85-106.

\bibitem[\protect\citename{Wong and Mooney}2006]{wong06}
Y.W. Wong and R.J. Mooney.
\newblock 2006.
\newblock {\em Learning for Semantic Parsing with Statistical Machine Translation}.
\newblock Proceedings of HLT/NAACL.

\bibitem[\protect\citename{Kate}2005]{kate05}
R.J. Kate, Y.W. Wong and R.J. Mooney.
\newblock 2008.
\newblock {\em Learning to Transform Natural to Formal Languages}.
\newblock Proceedings of AAAI.

\bibitem[\protect\citename{Kate}2008]{kate08}
R.J. Kate.
\newblock 2008.
\newblock {\em A Dependency-based Word Subsequence Kernel}.
\newblock Proceedings of EMNLP.

\bibitem[\protect\citename{Mairesse et al}2009]{mairesse09}
F. Mairesse, M. Gasic, F. Jurcicek, S. Keizer, B. Thomson, K. Yu, and S. Young.
\newblock 2009.
\newblock {\em Spoken Language Understanding from Unaligned Data using Discriminative Classification Models}.
\newblock Proceedings of ICASSP.

\bibitem[\protect\citename{Meza et al}2008a]{meza08a}
I.V. Meza-Ruiz, S. Riedel and O. Lemon.
\newblock 2008.
\newblock {\em Accurate statistical spoken language understanding from limited development resources}.
\newblock Proceedings of ICASSP.

\bibitem[\protect\citename{Meza et al}2008b]{meza08b}
I.V. Meza-Ruiz, S. Riedel and O. Lemon.
\newblock 2008.
\newblock {\em Spoken Language Understanding in dialogue systems, using a 2-layer Markov Logic Network: improving semantic accuracy}.
\newblock Proceedings of Londial.

\bibitem[\protect\citename{Tang et al}2001]{tang01}
L.R. Tang and R. J. Mooney 
\newblock 2001.
\newblock {\em Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing}.
\newblock Proceedings of ECML.

\bibitem[\protect\citename{Thomson et al}2008]{thomson08}
B. Thomson, M. Gasic, S. Keizer, F. Mairesse, J. Schatzmann,
K. Yu, and S. Young.
\newblock 2008.
\newblock {\em User study of the Bayesian update of dialogue state approach to dialogue management}.
\newblock Proceedings of Interspeech.

\bibitem[\protect\citename{Ward}1991]{ward91}
W.H. Ward.
\newblock 1991.
\newblock {\em The Phoenix system: Understanding spontaneous
speech}.
\newblock Proceedings of ICASSP.

\bibitem[\protect\citename{Williams and Young}2007]{williams07}
J. Williams and S. Young.
\newblock 2007.
\newblock {\em Partially observable markov decision
processes for spoken dialog systems}.
\newblock Computer Speech and Language, vol. 21, no. 2, pp. 231-422.

\bibitem[\protect\citename{Zettlemoyer and Collins}2007]{zettlemoyer07}
L.S. Zettlemoyer and M. Collins.
\newblock 2005.
\newblock {\em Online learning of relaxed CCG grammars for parsing to logical form}.
\newblock Proceedings of EMNLP-CoNLL.

\end{thebibliography}

\end{document}
